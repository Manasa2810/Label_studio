{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manasa2810/Label_studio/blob/main/Task3/stt_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dgk4opX7km66",
        "outputId": "5a6d2637-7833-41cf-bd8b-d96b7718c88f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (0.14.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install numpy pandas scikit-learn statsmodels\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# File paths\n",
        "annotator1_file = \"/content/anno1_nlp.csv\"\n",
        "annotator2_file = \"/content/ann2_nlp.csv\"\n",
        "\n",
        "# Loading data\n",
        "annotator1_df = pd.read_csv(annotator1_file, header=None)\n",
        "annotator2_df = pd.read_csv(annotator2_file, header=None)\n",
        "\n",
        "\n",
        "annotator1_df.columns = ['annotation_id', 'annotator', 'created_at', 'id', 'lead_time', 'pos_tags', 'text', 'updated_at']\n",
        "annotator2_df.columns = ['annotation_id', 'annotator', 'created_at', 'id', 'label', 'lead_time', 'text', 'updated_at']\n",
        "\n",
        "\n",
        "def safe_json_loads(s):\n",
        "    try:\n",
        "        return json.loads(s)\n",
        "    except (json.JSONDecodeError, TypeError):\n",
        "        return []\n",
        "\n",
        "# Parse annotations\n",
        "annotator1_df['parsed_pos_tags'] = annotator1_df['pos_tags'].apply(safe_json_loads)\n",
        "annotator2_df['parsed_label'] = annotator2_df['label'].apply(safe_json_loads)\n",
        "\n",
        "# Flatten and align annotations\n",
        "annotator1_labels = []\n",
        "annotator2_labels = []\n",
        "\n",
        "for i in range(len(annotator1_df)):\n",
        "    a1_tokens = annotator1_df['parsed_pos_tags'].iloc[i]\n",
        "    a2_tokens = annotator2_df['parsed_label'].iloc[i]\n",
        "\n",
        "    # Create dictionaries for alignment\n",
        "    a1_dict = {f\"{token['start']}_{token['end']}\": token['labels'][0].strip('\"') for token in a1_tokens}\n",
        "    a2_dict = {f\"{token['start']}_{token['end']}\": token['labels'][0].strip('\"') for token in a2_tokens}\n",
        "\n",
        "    # Align and compare tokens\n",
        "    for position, label in a1_dict.items():\n",
        "        if position in a2_dict:\n",
        "            annotator1_labels.append(label)\n",
        "            annotator2_labels.append(a2_dict[position])\n",
        "\n",
        "# Check for empty labels\n",
        "if not annotator1_labels or not annotator2_labels:\n",
        "    print(\"Error: No aligned labels found. Please check your data!\")\n",
        "else:\n",
        "    # Calculate Cohen's Kappa\n",
        "    kappa = cohen_kappa_score(annotator1_labels, annotator2_labels)\n",
        "\n",
        "    # Interpreting Cohen's Kappa\n",
        "    def interpret_kappa(kappa_score):\n",
        "        if kappa_score < 0:\n",
        "            return \"Poor agreement.\"\n",
        "        elif 0.01 <= kappa_score <= 0.20:\n",
        "            return \"Slight agreement.\"\n",
        "        elif 0.21 <= kappa_score <= 0.40:\n",
        "            return \"Fair agreement.\"\n",
        "        elif 0.41 <= kappa_score <= 0.60:\n",
        "            return \"Moderate agreement.\"\n",
        "        elif 0.61 <= kappa_score <= 0.80:\n",
        "            return \"Substantial agreement.\"\n",
        "        elif 0.81 <= kappa_score <= 1.00:\n",
        "            return \"Almost perfect agreement.\"\n",
        "        else:\n",
        "            return \"Invalid value.\"\n",
        "\n",
        "\n",
        "    print(f\"Cohen's Kappa: {kappa}\")\n",
        "    print(f\"Interpretation: {interpret_kappa(kappa)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDF1D8m1kymS",
        "outputId": "75579a53-1a01-4b7a-b96a-3c73f277355d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa: 0.8399662346341206\n",
            "Interpretation: Almost perfect agreement.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the annotator 3 CSV file\n",
        "ann3_df = pd.read_csv('/content/ann3.csv')\n",
        "\n",
        "# Start new annotation IDs from the desired number (e.g., 60)\n",
        "starting_id = 21\n",
        "ann3_df['annotation_id'] = range(starting_id, starting_id + len(ann3_df))\n",
        "\n",
        "# Save the updated annotator 3 file\n",
        "ann3_df.to_csv('updated_ann3_file.csv', index=False)\n",
        "\n",
        "print(\"Updated annotation IDs saved in 'updated_ann3_file.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-cWgyhPJe-0",
        "outputId": "fbba4ffc-dceb-4366-819a-a1d64f51f1d7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated annotation IDs saved in 'updated_ann3_file.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from statsmodels.stats.inter_rater import fleiss_kappa\n",
        "\n",
        "# File paths for the three annotators' data\n",
        "file1 = \"/content/ann1_clp.csv\"  # Replace with the path to annotator 1's file\n",
        "file2 = \"/content/ann2_cv.csv\"  # Replace with the path to annotator 2's file\n",
        "file3 = \"/content/updated_ann3_file.csv\"  # Replace with the path to annotator 3's file\n",
        "\n",
        "# Load data\n",
        "annotator1_df = pd.read_csv(file1)\n",
        "annotator2_df = pd.read_csv(file2)\n",
        "annotator3_df = pd.read_csv(file3)\n",
        "\n",
        "# Merge datasets on 'annotation_id' (assuming this column exists in all files)\n",
        "merged_df = annotator1_df[['annotation_id', 'choice']].rename(columns={'choice': 'annotator1'}).merge(\n",
        "    annotator2_df[['annotation_id', 'choice']].rename(columns={'choice': 'annotator2'}),\n",
        "    on='annotation_id'\n",
        ").merge(\n",
        "    annotator3_df[['annotation_id', 'choice']].rename(columns={'choice': 'annotator3'}),\n",
        "    on='annotation_id'\n",
        ")\n",
        "\n",
        "# Categories and build category matrix\n",
        "categories = pd.concat([merged_df['annotator1'], merged_df['annotator2'], merged_df['annotator3']]).unique()\n",
        "category_matrix = []\n",
        "\n",
        "for _, row in merged_df.iterrows():\n",
        "    # Count occurrences of each category for the current item\n",
        "    counts = pd.Series([row['annotator1'], row['annotator2'], row['annotator3']]).value_counts()\n",
        "    row_matrix = [counts.get(category, 0) for category in categories]\n",
        "    category_matrix.append(row_matrix)\n",
        "\n",
        "# Convert to DataFrame for easier debugging\n",
        "category_matrix_df = pd.DataFrame(category_matrix, columns=categories)\n",
        "print(\"Category Matrix:\")\n",
        "print(category_matrix)\n",
        "print(category_matrix_df)\n",
        "\n",
        "# Calculate Fleiss' Kappa\n",
        "kappa = fleiss_kappa(category_matrix, method='fleiss')\n",
        "\n",
        "# Interpretation\n",
        "def interpret_kappa(kappa_score):\n",
        "    if kappa_score < 0:\n",
        "        return \"Poor agreement.\"\n",
        "    elif 0.01 <= kappa_score <= 0.20:\n",
        "        return \"Slight agreement.\"\n",
        "    elif 0.21 <= kappa_score <= 0.40:\n",
        "        return \"Fair agreement.\"\n",
        "    elif 0.41 <= kappa_score <= 0.60:\n",
        "        return \"Moderate agreement.\"\n",
        "    elif 0.61 <= kappa_score <= 0.80:\n",
        "        return \"Substantial agreement.\"\n",
        "    elif 0.81 <= kappa_score <= 1.00:\n",
        "        return \"Almost perfect agreement.\"\n",
        "    else:\n",
        "        return \"Invalid value.\"\n",
        "\n",
        "# Output Fleiss' Kappa and interpretation\n",
        "print(f\"Fleiss' Kappa: {kappa}\")\n",
        "print(f\"Interpretation: {interpret_kappa(kappa)}\")\n"
      ],
      "metadata": {
        "id": "QCZze1UDIKLb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a74606c6-48d3-4540-fbaf-b1efb06d28a9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category Matrix:\n",
            "[[1, 2], [3, 0], [0, 3], [0, 3], [0, 3], [2, 1], [1, 2], [1, 2], [1, 2], [3, 0], [3, 0], [1, 2], [2, 1], [0, 3], [3, 0], [0, 3], [0, 3], [2, 1], [3, 0], [3, 0]]\n",
            "    Trucks  No Trucks\n",
            "0        1          2\n",
            "1        3          0\n",
            "2        0          3\n",
            "3        0          3\n",
            "4        0          3\n",
            "5        2          1\n",
            "6        1          2\n",
            "7        1          2\n",
            "8        1          2\n",
            "9        3          0\n",
            "10       3          0\n",
            "11       1          2\n",
            "12       2          1\n",
            "13       0          3\n",
            "14       3          0\n",
            "15       0          3\n",
            "16       0          3\n",
            "17       2          1\n",
            "18       3          0\n",
            "19       3          0\n",
            "Fleiss' Kappa: 0.4660734149054503\n",
            "Interpretation: Moderate agreement.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from statsmodels.stats.inter_rater import fleiss_kappa\n",
        "\n",
        "# File paths for the two annotators' data\n",
        "file1 = \"/content/ann1_clp.csv\"  # Replace with the path to annotator 1's file\n",
        "file2 = \"/content/ann2_cv.csv\"  # Replace with the path to annotator 2's file\n",
        "\n",
        "# Load data\n",
        "annotator1_df = pd.read_csv(file1)\n",
        "annotator2_df = pd.read_csv(file2)\n",
        "\n",
        "# Merge datasets on 'annotation_id' (assuming this column exists in both files)\n",
        "merged_df = annotator1_df[['annotation_id', 'choice']].rename(columns={'choice': 'annotator1'}).merge(\n",
        "    annotator2_df[['annotation_id', 'choice']].rename(columns={'choice': 'annotator2'}),\n",
        "    on='annotation_id'\n",
        ")\n",
        "\n",
        "# Extract unique categories\n",
        "categories = pd.concat([merged_df['annotator1'], merged_df['annotator2']]).unique()\n",
        "\n",
        "# Build category matrix\n",
        "category_matrix = []\n",
        "\n",
        "for _, row in merged_df.iterrows():\n",
        "    # Count occurrences of each category for the current item\n",
        "    counts = pd.Series([row['annotator1'], row['annotator2']]).value_counts()\n",
        "    row_matrix = [counts.get(category, 0) for category in categories]\n",
        "    category_matrix.append(row_matrix)\n",
        "\n",
        "# Convert to DataFrame for easier debugging\n",
        "category_matrix_df = pd.DataFrame(category_matrix, columns=categories)\n",
        "print(\"Category Matrix:\")\n",
        "print(category_matrix_df)\n",
        "\n",
        "# Calculate Fleiss' Kappa\n",
        "kappa = fleiss_kappa(category_matrix, method='fleiss')\n",
        "\n",
        "# Interpretation\n",
        "def interpret_kappa(kappa_score):\n",
        "    if kappa_score < 0:\n",
        "        return \"Poor agreement.\"\n",
        "    elif 0.01 <= kappa_score <= 0.20:\n",
        "        return \"Slight agreement.\"\n",
        "    elif 0.21 <= kappa_score <= 0.40:\n",
        "        return \"Fair agreement.\"\n",
        "    elif 0.41 <= kappa_score <= 0.60:\n",
        "        return \"Moderate agreement.\"\n",
        "    elif 0.61 <= kappa_score <= 0.80:\n",
        "        return \"Substantial agreement.\"\n",
        "    elif 0.81 <= kappa_score <= 1.00:\n",
        "        return \"Almost perfect agreement.\"\n",
        "    else:\n",
        "        return \"Invalid value.\"\n",
        "\n",
        "# Output Fleiss' Kappa and interpretation\n",
        "print(f\"Fleiss' Kappa: {kappa}\")\n",
        "print(f\"Interpretation: {interpret_kappa(kappa)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6sv9ykKI5UJ",
        "outputId": "29e41dec-78d1-46d0-8b53-b5bc9df9657a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category Matrix:\n",
            "    Trucks  No Trucks\n",
            "0        1          1\n",
            "1        2          0\n",
            "2        0          2\n",
            "3        0          2\n",
            "4        0          2\n",
            "5        1          1\n",
            "6        1          1\n",
            "7        0          2\n",
            "8        0          2\n",
            "9        2          0\n",
            "10       2          0\n",
            "11       1          1\n",
            "12       1          1\n",
            "13       0          2\n",
            "14       2          0\n",
            "15       0          2\n",
            "16       0          2\n",
            "17       1          1\n",
            "18       2          0\n",
            "19       2          0\n",
            "Fleiss' Kappa: 0.3939393939393937\n",
            "Interpretation: Fair agreement.\n"
          ]
        }
      ]
    }
  ]
}